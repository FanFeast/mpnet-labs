from typing import Dict

import torch
import torch.nn.functional as F
from mpnet_dataset import MPNetToyDataset
from mpnet_model import MPNet2D
from torch.utils.data import DataLoader, random_split


def train_one_epoch(
    model: MPNet2D,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
) -> float:
    model.train()
    total_loss = 0.0
    total_samples = 0

    for batch in loader:
        grid = batch["grid"].to(device)  # (B, 1, H, W)
        x_cur = batch["x_cur"].to(device)  # (B, 2)
        x_goal = batch["x_goal"].to(device)  # (B, 2)
        x_next = batch["x_next"].to(device)  # (B, 2)

        optimizer.zero_grad()
        z = model.encode_env(grid)
        pred_next = model.step(z, x_cur, x_goal)
        loss = F.mse_loss(pred_next, x_next)
        loss.backward()
        optimizer.step()

        batch_size = grid.size(0)
        total_loss += loss.item() * batch_size
        total_samples += batch_size

    return total_loss / max(total_samples, 1)


@torch.no_grad()
def eval_one_epoch(
    model: MPNet2D,
    loader: DataLoader,
    device: torch.device,
) -> float:
    model.eval()
    total_loss = 0.0
    total_samples = 0

    for batch in loader:
        grid = batch["grid"].to(device)
        x_cur = batch["x_cur"].to(device)
        x_goal = batch["x_goal"].to(device)
        x_next = batch["x_next"].to(device)

        z = model.encode_env(grid)
        pred_next = model.step(z, x_cur, x_goal)
        loss = F.mse_loss(pred_next, x_next)

        batch_size = grid.size(0)
        total_loss += loss.item() * batch_size
        total_samples += batch_size

    return total_loss / max(total_samples, 1)


def main() -> None:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using device:", device)

    dataset_path = "mpnet_toy_dataset.npz"  # generated by your dataset_gen
    full_dataset = MPNetToyDataset(dataset_path)

    # Simple 90/10 train/val split
    n_total = len(full_dataset)
    n_train = int(0.9 * n_total)
    n_val = n_total - n_train
    train_ds, val_ds = random_split(full_dataset, [n_train, n_val])

    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=128, shuffle=False)

    model = MPNet2D(latent_dim=64, hidden_dim=128).to(device)
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=1e-3,
        weight_decay=1e-5,
    )

    num_epochs = 20

    for epoch in range(1, num_epochs + 1):
        train_loss = train_one_epoch(model, train_loader, optimizer, device)
        val_loss = eval_one_epoch(model, val_loader, device)

        print(f"Epoch {epoch:02d} train_loss={train_loss:.6f} val_loss={val_loss:.6f}")

    # Save trained weights
    torch.save(model.state_dict(), "mpnet_toy_model.pt")
    print("Saved model to mpnet_toy_model.pt")


if __name__ == "__main__":
    main()
