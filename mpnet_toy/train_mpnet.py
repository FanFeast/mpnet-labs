from typing import Dict

import torch
import torch.backends.cudnn as cudnn
import torch.nn.functional as F
from mpnet_dataset import MPNetToyDataset
from mpnet_model import MPNet2D
from torch.amp import GradScaler, autocast
from torch.utils.data import DataLoader, random_split


def train_one_epoch(
    model: MPNet2D,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scaler: GradScaler,
    device: torch.device,
) -> float:
    model.train()
    total_loss = 0.0
    total_samples = 0

    for batch in loader:
        grid = batch["grid"].to(device, non_blocking=True)  # (B, 1, H, W)
        x_cur = batch["x_cur"].to(device, non_blocking=True)  # (B, 2)
        x_goal = batch["x_goal"].to(device, non_blocking=True)  # (B, 2)
        x_next = batch["x_next"].to(device, non_blocking=True)  # (B, 2)

        optimizer.zero_grad()

        with autocast('cuda'):
            z = model.encode_env(grid)
            pred_next = model.step(z, x_cur, x_goal)
            loss = F.mse_loss(pred_next, x_next)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        batch_size = grid.size(0)
        total_loss += loss.item() * batch_size
        total_samples += batch_size

    return total_loss / max(total_samples, 1)


@torch.no_grad()
def eval_one_epoch(
    model: MPNet2D,
    loader: DataLoader,
    device: torch.device,
) -> float:
    model.eval()
    total_loss = 0.0
    total_samples = 0

    for batch in loader:
        grid = batch["grid"].to(device, non_blocking=True)
        x_cur = batch["x_cur"].to(device, non_blocking=True)
        x_goal = batch["x_goal"].to(device, non_blocking=True)
        x_next = batch["x_next"].to(device, non_blocking=True)

        with autocast('cuda'):
            z = model.encode_env(grid)
            pred_next = model.step(z, x_cur, x_goal)
            loss = F.mse_loss(pred_next, x_next)

        batch_size = grid.size(0)
        total_loss += loss.item() * batch_size
        total_samples += batch_size

    return total_loss / max(total_samples, 1)


def main() -> None:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using device:", device)

    if device.type == "cuda":
        cudnn.benchmark = True

    dataset_path = "mpnet_toy_dataset.npz"  # generated by your dataset_gen
    full_dataset = MPNetToyDataset(dataset_path)

    # Simple 90/10 train/val split
    n_total = len(full_dataset)
    n_train = int(0.9 * n_total)
    n_val = n_total - n_train
    train_ds, val_ds = random_split(full_dataset, [n_train, n_val])

    train_loader = DataLoader(
        train_ds, batch_size=512, shuffle=True, num_workers=4, pin_memory=True
    )
    val_loader = DataLoader(
        val_ds, batch_size=512, shuffle=False, num_workers=4, pin_memory=True
    )

    model = MPNet2D(latent_dim=64, hidden_dim=128).to(device)

    # Compile submodules since we call them directly
    if hasattr(torch, "compile"):
        print("Compiling model...")
        model.encoder = torch.compile(model.encoder)
        model.planner = torch.compile(model.planner)

    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=1e-3,
        weight_decay=1e-5,
    )

    scaler = GradScaler('cuda')

    num_epochs = 100

    for epoch in range(1, num_epochs + 1):
        train_loss = train_one_epoch(model, train_loader, optimizer, scaler, device)
        val_loss = eval_one_epoch(model, val_loader, device)

        print(f"Epoch {epoch:02d} train_loss={train_loss:.6f} val_loss={val_loss:.6f}")

    # Save trained weights
    torch.save(model.state_dict(), "mpnet_toy_model.pt")
    print("Saved model to mpnet_toy_model.pt")


if __name__ == "__main__":
    main()
