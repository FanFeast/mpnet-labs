"""
Quick evaluation script for a trained MPNet2D checkpoint.

Loads a saved model, runs it on the saved toy dataset, and reports a few
numerical metrics so you can sanity-check changes to the network.
"""
from __future__ import annotations

import argparse
from typing import Dict

import torch
import torch.nn.functional as F
from mpnet_dataset import MPNetToyDataset
from mpnet_model import MPNet2D
from torch.amp import autocast
from torch.utils.data import DataLoader


@torch.no_grad()
def evaluate(
    model: MPNet2D,
    loader: DataLoader,
    device: torch.device,
) -> Dict[str, float]:
    """
    Evaluate the model on the provided DataLoader.

    Returns a dict with a few aggregate metrics to make regression testing easy:
      - coord_mse : mean squared error per coordinate
      - coord_mae : mean absolute error per coordinate
      - l2_mean   : mean Euclidean error per step
    """
    model.eval()

    mse_sum = 0.0
    mae_sum = 0.0
    l2_sum = 0.0
    total_samples = 0

    for batch in loader:
        grid = batch["grid"].to(device, non_blocking=True)  # (B, 1, H, W)
        x_cur = batch["x_cur"].to(device, non_blocking=True)  # (B, 2)
        x_goal = batch["x_goal"].to(device, non_blocking=True)  # (B, 2)
        x_next = batch["x_next"].to(device, non_blocking=True)  # (B, 2)

        with autocast(device_type=device.type, enabled=device.type == "cuda"):
            z = model.encode_env(grid)
            pred_next = model.step(z, x_cur, x_goal)

        # Sum of squared error across both coordinates for each sample
        mse_sum += F.mse_loss(pred_next, x_next, reduction="sum").item()
        # Sum of absolute error across both coordinates
        mae_sum += F.l1_loss(pred_next, x_next, reduction="sum").item()
        # Euclidean error per sample
        l2_sum += torch.linalg.norm(pred_next - x_next, dim=-1).sum().item()
        total_samples += x_next.size(0)

    # Each sample has 2 coordinates
    coord_den = max(total_samples * 2, 1)

    metrics = {
        "coord_mse": mse_sum / coord_den,
        "coord_rmse": (mse_sum / coord_den) ** 0.5,
        "coord_mae": mae_sum / coord_den,
        "l2_mean": l2_sum / max(total_samples, 1),
    }
    return metrics


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Evaluate MPNet2D checkpoint.")
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="mpnet_toy_model.pt",
        help="Path to the saved model weights.",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="mpnet_toy_dataset.npz",
        help="Path to the .npz dataset generated by dataset_gen.py.",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=512,
        help="Batch size for evaluation.",
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=4,
        help="DataLoader workers.",
    )
    parser.add_argument(
        "--latent-dim",
        type=int,
        default=64,
        help="Latent dimension used when creating MPNet2D.",
    )
    parser.add_argument(
        "--hidden-dim",
        type=int,
        default=128,
        help="Hidden dimension for the planning MLP.",
    )
    parser.add_argument(
        "--cpu",
        action="store_true",
        help="Force CPU even if CUDA is available.",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")
    print("Using device:", device)

    dataset = MPNetToyDataset(args.dataset)
    loader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=device.type == "cuda",
    )

    model = MPNet2D(
        latent_dim=args.latent_dim,
        hidden_dim=args.hidden_dim,
    ).to(device)

    raw_state = torch.load(args.checkpoint, map_location=device)
    clean_state = {
        k.replace("._orig_mod.", "."): v
        for k, v in raw_state.items()
    }
    missing, unexpected = model.load_state_dict(clean_state, strict=False)
    if missing or unexpected:
        print("Warning: load_state_dict mismatches")
        print("  missing:", missing)
        print("  unexpected:", unexpected)

    metrics = evaluate(model, loader, device)
    print("Evaluation metrics:")
    for k, v in metrics.items():
        print(f"  {k}: {v:.6f}")


if __name__ == "__main__":
    main()
